# LeFusion: Lesion-Focused Diffusion Model

LeFusion has been accepted by [ICLR'25](https://openreview.net/forum?id=3b9SKkRAKw) (Spotlight).

![lefusion_model](https://github.com/M3DV/LeFusion/blob/main/figures/lefusion_model.png)

The top illustrates the training process of LeFusion, while the bottom shows the inference. During training, LeFusion avoids learning unnecessary background generation using a lesion-focused loss. In inference, by combining forward-diffused real backgrounds with reverse-diffused generated foregrounds, LeFusion ensures high-quality background generation. Additionally, we introduce histogram-based texture control to handle multi-peak lesions and multi-channel decomposition for multi-class lesions. ([arXiv](https://arxiv.org/abs/2403.14066))


## :bookmark_tabs:Data Preparation

We utilized the [LIDC](https://pubmed.ncbi.nlm.nih.gov/21452728/) dataset, which includes 1,010 chest CT scans. From these, we extracted 2,624 pathology regions of interest (ROIs) related to lung nodules to train the LeFusion Model. The dataset is divided into 808 cases for training, containing 2,104 lung nodule ROIs, and 202 cases for testing, containing 520 lung nodule ROIs. This portion of the dataset is located in `LIDC-IDRI\Pathological`, with the `test.txt` listing the data used for testing.

Additionally, we provide 20 normal ROIs from healthy patients, representing areas where lung nodules typically appear. This data is located in `LIDC-IDRI\Normal`, where `Image` contains the healthy images, and `Mask` includes the corresponding masks generated by matching lung and ground truth masks, which can be used to generate lesions. You can simulate lesion generation on the Normal dataset.

Furthermore, we provide pre-generated images with lesions based on the `LIDC-IDRI\Normal` dataset. These images are stored in `LIDC-IDRI\Demo`, where `Image_i` represents the images generated under the control information *hist_i*. The pre-trained weights used to generate these images are available in the pre-trained weights mentioned below.

```
â”œâ”€â”€ LIDC-IDRI
    â”œâ”€â”€ Pathological
    â”‚   â”œâ”€â”€ Image
    â”‚   â”œâ”€â”€ Mask
    â”‚   â””â”€â”€ test.txt
    â”œâ”€â”€ Normal
    â”‚   â”œâ”€â”€ Image
    â”‚   â””â”€â”€ Mask
    â””â”€â”€ Demo
        â”œâ”€â”€ Image
        â”‚   â”œâ”€â”€ Image_1
        â”‚   â”œâ”€â”€ Image_2
        â”‚   â””â”€â”€ Image_3
        â””â”€â”€ Mask
            â”œâ”€â”€ Mask_1
            â”œâ”€â”€ Mask_2
            â””â”€â”€ Mask_3
```

Besides, we provide the preprocessed [EMIDEC](https://www.mdpi.com/2306-5729/5/4/89) dataset, which contains 57 pathology MRI scans and 43 healthy MRI scans.

```
â”œâ”€â”€ EMIDEC
    â”œâ”€â”€ Pathological
    â”‚   â”œâ”€â”€ images
    â”‚   â”œâ”€â”€ labels
    â””â”€â”€ Normal
        â””â”€â”€ images
```
## :nut_and_bolt: Installation

1. Create a virtual environment `conda create -n lefusion python=3.10` and activate it `conda activate lefusion`
2. Download the code`git clone https://github.com/M3DV/LeFusion.git`
3. Check if your pip version is 22.3.1. If it is not, install pip version 22.3.1 `pip install pip==22.3.1`
4. Enter the LeFusion folder `cd LeFusion/LeFusion_LIDC` and run `pip install -r requirements.txt`

## :bulb:Get Started

1. Download the LIDC_IDRI and EMIDEC dataset ([HuggingFaceðŸ¤—](https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/tree/main))

   In our study, the LeFusion Model focuses on the generation of lung nodule regions. If you want to train a Diffusion Model to synthesize lung nodules, you can use the LIDC-IDRI dataset that has already been processed by us to train the LeFusion Model. Just put the LIDC-IDRI dataset to `LeFusion/data`.

   ```bash
   mkdir data
   cd data
   mkdir LIDC
   cd LIDC
   wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Pathological.tar -O Pathological.tar
   tar -xvf Pathological.tar
   wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Normal.tar -O Normal.tar
   tar -xvf Normal.tar
   wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/LIDC-IDRI/Demo.tar -O Demo.tar
   tar -xvf Demo.tar
   ``` 
   
   Additionally, if you wish to train the LeFusion Model using the EMIDEC dataset, you can download the dataset as follow.

   ```bash     
   cd ..
   mkdir EMIDEC
   cd EMIDEC
   wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Normal.tar -O Normal.tar
   tar -xvf Normal.tar
   wget https://huggingface.co/datasets/YuheLiuu/LeFusion_Preprocessed_Data/resolve/main/EMIDEC/Pathological.tar -O Pathological.tar
   tar -xvf Pathological.tar

   ```

2. Download the pre-trained LeFusion Model ([HuggingFaceðŸ¤—](https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/tree/main))

   We provide pre-trained models on the LIDC and EMIDEC datasets. This pre-trained model can be directly used for Inference if you do not want to re-train the LeFusion Model.

   ```bash
   cd ../..
   cd LeFusion
   mkdir LeFusion_Model
   cd LeFusion_Model
   mkdir LIDC
   cd LIDC
   wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/lidc.pt -O lidc.pt
   cd ..
   mkdir EMIDEC
   cd EMIDEC
   wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/emidec.pt -O emidec.pt
   ```

   If you have downloaded the pre-trained model, you can skip the training step and proceed directly to inference!

## :microscope:Train LeFusion Model

Start training:

For LIDC:

```bash
chmod +x lidc_train.sh
./lidc_train.sh
```

Our model was trained for 50,000 steps using five 40GB A100 GPUs, taking two and a half days. However, we found that the model performs very well after 20,000 steps. Therefore, when training a model on your own, anywhere between 20,000 to 50,000 steps would yield good results. Additionally, by default, we save the weights every 1,000 steps.

For EMIDEC:

```bash
chmod +x emidec_train.sh
./emidec_train.sh
```

## :chart_with_upwards_trend:Inference

Start inference:

For LIDC:

```bash
chmod +x lidc_inference.sh
./lidc_inference.sh
```

Three folders, Image_1, Image_2, and Image_3, will be generated under the` target_img_path` directory, each representing images generated under the control of hist_1, hist_2, and hist_3 respectively. Similarly, three folders will be generated under the Mask directory, but unlike the Image folders, files with the same name in each of the three Mask folders contain the same mask.

For *jump_length* and *jump_n_sample*, larger values generally result in longer image generation times. We found that when these two parameters are between 2 and 10, the generated images maintain good quality. When both parameters are set to 2, it takes about 40 seconds to generate an image using a 40G A100 GPU.

For EMIDEC:

```bash
chmod +x emidec_inference.sh
./emidec_inference.sh
```

## :mag_right:Visualization

![visualization](https://github.com/M3DV/LeFusion/blob/main/figures/visualization.jpg)
The first image is a healthy image from `LIDC-IDRI/Normal`. The second image is the corresponding generated mask, where lesions will be generated in the areas marked by the mask. Image_1, Image_2, and Image_3 are the lesions generated when the control information is set to Hist_1, Hist_2, and Hist_3, respectively.

## :test_tube:Evaluation Pipeline

The evaluation pipeline provides a comprehensive framework to assess the effectiveness of LeFusion-generated synthetic data for medical image segmentation tasks. This evaluation follows established protocols in medical AI research and uses standard metrics to measure performance.

### Overview

The evaluation process consists of three main steps:
1. **Synthetic Data Generation**: Generate synthetic pathological data using LeFusion
2. **Model Training**: Train segmentation models on combined real + synthetic data
3. **Performance Evaluation**: Evaluate models on held-out real pathological data

### Prerequisites

Before running the evaluation, ensure you have:
- âœ… LeFusion environment activated (`conda activate lefusion`)
- âœ… LIDC dataset downloaded and organized in `data/LIDC/`
- âœ… Pre-trained LeFusion model downloaded
- âœ… Required dependencies installed

### Quick Start

#### Option 1: Full Automated Pipeline

```bash
cd evaluation_pipeline
chmod +x run_monai_pipeline.sh
./run_monai_pipeline.sh
```

This script will:
- Generate synthetic data using LeFusion
- Train a SwinUNETR model on combined data
- Save results to `evaluation_results.csv`

#### Option 2: Step-by-Step Evaluation

**Step 1: Prepare Real Dataset**
```bash
cd evaluation_pipeline
python prepare_real_dataset.py \
    --source_image_dir "../data/LIDC/Pathological/Image" \
    --source_mask_dir "../data/LIDC/Pathological/Mask" \
    --test_txt_path "../data/LIDC/Pathological/test.txt" \
    --output_dir "datasets/LIDC_real"
```

**Step 2: Generate Synthetic Data**
```bash
cd evaluation_pipeline
python ../LeFusion/inference/inference.py \
    data_type=lidc \
    model_path='../LeFusion/LeFusion_Model/LIDC/lidc.pt' \
    dataset_root_dir='../data/LIDC/Normal/Image' \
    test_txt_dir='../data/LIDC/Pathological/test.txt' \
    target_img_path="datasets/LeFusion_H_N_prime/imagesTr" \
    target_label_path="datasets/LeFusion_H_N_prime/labelsTr" \
    batch_size=4 \
    types=3
```

**Step 3: Train Segmentation Model**
```bash
python run_segmentation_training.py \
    --real_data_dir "datasets/LIDC_real" \
    --synthetic_data_dir "datasets/LeFusion_H_N_prime" \
    --model_name "SwinUNETR" \
    --output_model_dir "trained_models/LeFusion_H_P+N_prime_SwinUNETR"
```

**Step 4: Evaluate Model Performance**
```bash
python run_segmentation_evaluation.py \
    --test_data_dir "datasets/LIDC_real" \
    --gt_dir "datasets/LIDC_real/labelsTs" \
    --trained_model_path "trained_models/LeFusion_H_P+N_prime_SwinUNETR/best_model.pt" \
    --model_name "SwinUNETR" \
    --output_pred_dir "predictions" \
    --results_csv "evaluation_results.csv" \
    --experiment_name "LeFusion_H_P+N_prime"
```

### Supported Models

The evaluation pipeline supports multiple segmentation architectures:

- **SwinUNETR**: Transformer-based architecture with Swin attention
- **nnUNet**: Standard U-Net with nnU-Net preprocessing
- **U-Net**: Classic U-Net architecture

### Evaluation Metrics

The pipeline calculates standard medical image segmentation metrics:

- **Dice Score**: Measures overlap between predicted and ground truth masks
- **Hausdorff Distance (HD95)**: Measures surface distance accuracy
- **Normalized Surface Dice (NSD)**: Surface-based metric with tolerance

### Configuration Options

#### Model Parameters
```bash
# Choose your model architecture
MODEL_NAME="SwinUNETR"  # Options: SwinUNETR, nnUNet, UNet

# Training parameters
MAX_EPOCHS=200
BATCH_SIZE=2
LEARNING_RATE=1e-4
```

#### Data Parameters
```bash
# Synthetic data generation
METHOD_NAME="LeFusion-H"  # Your method name
BATCH_SIZE=4
TYPES=3  # Number of synthetic samples per normal image
```

#### Evaluation Parameters
```bash
# Validation settings
VAL_OVERLAP=0.75
CACHE_RATE=0.5
```

### Output Structure

After running the evaluation, you'll find:

```
evaluation_pipeline/
â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ LIDC_real/           # Real dataset in nnU-Net format
â”‚   â””â”€â”€ LeFusion_H_N_prime/  # Synthetic dataset
â”œâ”€â”€ trained_models/
â”‚   â””â”€â”€ LeFusion_H_P+N_prime_SwinUNETR/  # Trained models
â”œâ”€â”€ predictions/              # Model predictions
â”œâ”€â”€ evaluation_results.csv    # Final results
â””â”€â”€ logs/                     # Training logs
```

### Results Interpretation

The `evaluation_results.csv` file contains:
- **Experiment Name**: Method and configuration used
- **Model**: Architecture tested
- **Dice**: Dice score percentage
- **NSD_Proxy_HD95**: Hausdorff distance metric

### Advanced Usage

#### Cross-Validation
```bash
# Run 5-fold cross-validation
for fold in {0..4}; do
    python run_segmentation_training.py \
        --fold $fold \
        --real_data_dir "datasets/LIDC_real" \
        --synthetic_data_dir "datasets/LeFusion_H_N_prime" \
        --model_name "SwinUNETR"
done
```

#### Multiple Model Comparison
```bash
# Compare different architectures
for model in "SwinUNETR" "nnUNet" "UNet"; do
    python run_segmentation_training.py \
        --real_data_dir "datasets/LIDC_real" \
        --synthetic_data_dir "datasets/LeFusion_H_N_prime" \
        --model_name $model
done
```

#### Custom Evaluation
```bash
# Evaluate on custom test set
python run_segmentation_evaluation.py \
    --test_data_dir "your_custom_test_data" \
    --gt_dir "your_ground_truth" \
    --trained_model_path "path_to_your_model.pt" \
    --model_name "SwinUNETR"
```

### Troubleshooting

#### Common Issues

1. **CUDA Out of Memory**
   ```bash
   # Reduce batch size
   BATCH_SIZE=1
   ```

2. **Dataset Format Issues**
   ```bash
   # Ensure nnU-Net format
   python prepare_real_dataset.py --help
   ```

3. **Model Loading Errors**
   ```bash
   # Check model path and architecture
   ls -la trained_models/
   ```

#### Performance Tips

- Use GPU acceleration for faster training
- Adjust `cache_rate` based on available RAM
- Use mixed precision training for memory efficiency
- Monitor GPU memory usage during training

### Scientific Validation

This evaluation pipeline follows established protocols:
- âœ… **Proper train/test splits** to avoid data leakage
- âœ… **Standard medical AI metrics** (Dice, HD95, NSD)
- âœ… **Cross-validation** for reliable results
- âœ… **Multiple model architectures** for robustness
- âœ… **Statistical rigor** using MONAI framework

The evaluation results provide **scientifically valid evidence** of LeFusion's effectiveness for medical image segmentation tasks.

## :sunny: DiffMask

The training and inference process of DiffMask is as follows.

Get pre-trained DiffMask Model([HuggingFaceðŸ¤—](https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/tree/main)):

```bash
cd DiffMask
mkdir DiffMask_Model
cd DiffMask_Model
wget https://huggingface.co/YuheLiuu/LeFusion_Pretrained_model/resolve/main/diffmask.pt -O diffmask.pt
cd ../..
```

Train:

```bash
chmod +x diffmask_inference.sh
./diffmask_inference.sh
```

Infernce:

```bash
chmod +x diffmask_inference.sh
./diffmask_inference.sh
```


## Citation

```
@misc{zhang2024lefusioncontrollablepathologysynthesis,
      title={LeFusion: Controllable Pathology Synthesis via Lesion-Focused Diffusion Models}, 
      author={Hantao Zhang and Yuhe Liu and Jiancheng Yang and Shouhong Wan and Xinyuan Wang and Wei Peng and Pascal Fua},
      year={2024},
      eprint={2403.14066},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2403.14066}, 
}
```

## Acknowledgement

Some of our code is modified based on [medicaldiffusion](https://github.com/FirasGit/medicaldiffusion) and [RePaint](https://github.com/andreas128/RePaint), and we greatly appreciate the efforts of the respective authors for providing open-source code. We also thank [DiffTumor](https://github.com/MrGiovanni/DiffTumor/tree/main/STEP3.SegmentationModel) for providing the segmentation model code.


## Community Contribution: [3D Slicer Extension for LeFusion](https://github.com/pedr0sorio/lefusion-slicer)

For those who work with medical imaging and seek to bring LeFusion's inpainting model closer to real-world clinical practice, we are excited to introduce a community contribution: a [3D Slicer extension](https://github.com/pedr0sorio/lefusion-slicer)! This extension leverages our inpainting model as the backend, offering practical applications for radiologists and other medical professionals. 

Special thanks to [@pedr0sorio](https://github.com/pedr0sorio) for developing this valuable tool.

## ToDo List

âœ… **The preprocessed LIDC-IDRI dataset**  ðŸš€

âœ… **The LeFusion model applied to LIDC-IDRI** ðŸš€

âœ… **The DiffMask model used for generating mask** ðŸš€

âœ… **The preprocessed EMIDEC dataset**  ðŸš€

âœ… **The LeFusion model applied to EMIDEC**  ðŸš€
